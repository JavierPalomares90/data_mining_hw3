{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Javier Palomares\n",
    "### Problem 1\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’. Focus on pages 1-19 (up\n",
    "to Part II), the remaining part is more relevant for communication.\n",
    "http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "Summarize what you learned briefly (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, Shannon introduces entropy within the context of communication systems. The paper first introduces the channel used for transmitting information in the system. The channel is a system where a sequence of choices from a finite set $ \\{s_1,s_2,...,s_n\\}$ are transmitted from one point to another.  \n",
    "The system has a capacity $ C=\\lim_{T\\to \\inf} \\frac{N(T)}{T}$ where $N(T)$ is the number of allowed signals of length $T$. If all sequences of $ \\{s_1,s_2,...,s_n\\}$ are allowed and the symbols have duration $t_1,t_2,...,t_n$ then it's shown that $C = logX_0$ where $X_0$ is the largest solution of $X^{-t_1} + X ^{-t_2} + ... + X ^{-t_n} =1$.  \n",
    "This result is then taken to mean that the log of the number of possible signals increases linearly with time, and the capacity of a channel can be determined by the rate of increase. More importantly, this also means that statistical knowledge of the prodution of the sequence has the effect of reducing the required capacity of the channel.  \n",
    "So we want to define a quantity which will measure how much inofrmation is produced by the source of the sequence, and at what rate this information is produced. This quantity, $H$, should have the following properties for a set of possible events each with probability $ p_1, p_2,...,p_n$:\n",
    "\n",
    "1. $H$ should be continuous in the $p_i$\n",
    "2. If all the $p_i$ are equal, then $H$ should be a monotonic function of n.\n",
    "3. If a choice can be broken down into 2 successive choices, then the original $H$ should be the weighted sum of the individual H. $H(\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}) = H(\\frac{1}{2},\\frac{1}{2}) + \\frac{1}{2} H (\\frac{2}{3},\\frac{1}{3})$ \n",
    "If turns out that the only $H$ satisfying the 3 properites is of the form  \n",
    "$H = - K \\sum_{i=1}^{n} p_i * log p_i, K>0$\n",
    "\n",
    "This is the mathematical definition of what we call entropy for a discrete system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Scraping, Entropy and ICML papers.\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2018 papers\n",
    "from http://proceedings.mlr.press/v80/.\n",
    "1. What are the top 10 common words in the ICML papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
