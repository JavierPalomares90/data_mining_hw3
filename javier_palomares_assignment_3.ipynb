{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Javier Palomares\n",
    "### Problem 1\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’. Focus on pages 1-19 (up\n",
    "to Part II), the remaining part is more relevant for communication.\n",
    "http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "Summarize what you learned briefly (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, Shannon introduces entropy within the context of communication systems. The paper first introduces the channel used for transmitting information in the system. The channel is a system where a sequence of choices from a finite set $ \\{s_1,s_2,...,s_n\\}$ are transmitted from one point to another.  \n",
    "The system has a capacity $ C=\\lim_{T\\to \\inf} \\frac{N(T)}{T}$ where $N(T)$ is the number of allowed signals of length $T$. If all sequences of $ \\{s_1,s_2,...,s_n\\}$ are allowed and the symbols have duration $t_1,t_2,...,t_n$ then it's shown that $C = logX_0$ where $X_0$ is the largest solution of $X^{-t_1} + X ^{-t_2} + ... + X ^{-t_n} =1$.  \n",
    "This result is then taken to mean that the log of the number of possible signals increases linearly with time, and the capacity of a channel can be determined by the rate of increase. More importantly, this also means that statistical knowledge of the prodution of the sequence has the effect of reducing the required capacity of the channel.  \n",
    "So we want to define a quantity which will measure how much inofrmation is produced by the source of the sequence, and at what rate this information is produced. This quantity, $H$, should have the following properties for a set of possible events each with probability $ p_1, p_2,...,p_n$:\n",
    "\n",
    "1. $H$ should be continuous in the $p_i$\n",
    "2. If all the $p_i$ are equal, then $H$ should be a monotonic function of n.\n",
    "3. If a choice can be broken down into 2 successive choices, then the original $H$ should be the weighted sum of the individual H. $H(\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}) = H(\\frac{1}{2},\\frac{1}{2}) + \\frac{1}{2} H (\\frac{2}{3},\\frac{1}{3})$ \n",
    "If turns out that the only $H$ satisfying the 3 properites is of the form  \n",
    "$H = - K \\sum_{i=1}^{n} p_i * log p_i, K>0$\n",
    "\n",
    "This is the mathematical definition of what we call entropy for a discrete system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Scraping, Entropy and ICML papers.\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2018 papers\n",
    "from http://proceedings.mlr.press/v80/.\n",
    "1. What are the top 10 common words in the ICML papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfminer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ad6eb00753fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfdocument\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfpage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer'"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "URL = 'http://proceedings.mlr.press/v80/'\n",
    "DOWNLOAD_PATH = \"./pdfs/\"\n",
    "FREQUENCY_PATH = \"./freqs/\"\n",
    "\n",
    "\n",
    "def get_all_links():\n",
    "    # connect to the url\n",
    "    website = urlopen(URL)\n",
    "    # get the html code\n",
    "    html = website.read().decode('utf8')\n",
    "\n",
    "    # use re.findall to get all the links\n",
    "    links = re.findall('\"((http|ftp)s?://.*?)\"', html)\n",
    "    return links\n",
    "\n",
    "def get_pdf_links(links):\n",
    "    pdfs = []\n",
    "    pattern = re.compile(\".*\\.pdf\")\n",
    "    for tuple in links:\n",
    "        link = tuple[0]\n",
    "        matches = re.match(pattern,link)\n",
    "        if matches:\n",
    "            pdfs.append(link)\n",
    "    return pdfs\n",
    "\n",
    "def download_pdfs(pdfs):\n",
    "    for pdf_link in pdfs:\n",
    "        pdf = urlopen(pdf_link)\n",
    "        o = urlparse(pdf_link)\n",
    "        print(\"Downloading {}\".format(pdf_link))\n",
    "        file_name = os.path.basename(o[2])\n",
    "        path = DOWNLOAD_PATH + file_name\n",
    "        f = open(path,'wb+')\n",
    "        f.write(pdf.read())\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def read_pdfs():\n",
    "    pdf_files = [f for f in listdir(DOWNLOAD_PATH) if isfile(join(DOWNLOAD_PATH, f))]\n",
    "    word_freq = {}\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopWords.add('et')\n",
    "    stopWords.add('al')\n",
    "    for pdf_file in pdf_files:\n",
    "        # word frequency in this one file. we'll write the\n",
    "        # word frequency of per every single file in addition to\n",
    "        # the word frequency across all files\n",
    "        word_prob_this_file= {}\n",
    "        file_len = 0\n",
    "        print(\"reading {}\".format(pdf_file))\n",
    "\n",
    "        path = DOWNLOAD_PATH + pdf_file\n",
    "        fp = open(path,'rb')\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "        # set parameters for analysis\n",
    "        laparams = LAParams()\n",
    "\n",
    "        # Create a PDFDevice object which translates interpreted information into desired format\n",
    "        # Device needs to be connected to resource manager to store shared resources\n",
    "        # device = PDFDevice(rsrcmgr)\n",
    "        # Extract the decive to page aggregator to get LT object elements\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "        # Create interpreter object to process page content from PDFDocument\n",
    "        # Interpreter needs to be connected to resource manager for shared resources and device\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            # As the interpreter processes the page stored in PDFDocument object\n",
    "            interpreter.process_page(page)\n",
    "            # The device renders the layout from interpreter\n",
    "            layout = device.get_result()\n",
    "            # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "            for lt_obj in layout:\n",
    "                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    text = lt_obj.get_text()\n",
    "                    # remove any characters that pdfminer was not able to\n",
    "                    # convert to unicode and converts to (cid:%%)\n",
    "                    text = re.sub('(cid:[0-9]+)','',text)\n",
    "                    # make all text lowercase\n",
    "                    text = text.lower()\n",
    "                    # remove line breaks\n",
    "                    text = text.replace('-\\n','')\n",
    "                    # remove all punctuation from the text\n",
    "                    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "                    # remove new line\n",
    "                    text = text.replace('\\n',' ')\n",
    "                    text = text.replace('−',' ')\n",
    "\n",
    "                    extracted_text += text\n",
    "        words = word_tokenize(extracted_text)\n",
    "\n",
    "        for word in words:\n",
    "            # don't include stop words or numeric words or single chars representing mathematical variables\n",
    "            if (word not in stopWords) and (word.isnumeric()==False) and (len(word) > 1):\n",
    "                # take all word to lowercase\n",
    "                word = word.lower()\n",
    "                # frequency across all files\n",
    "                count = word_freq.get(word,0)\n",
    "                word_freq[word] = count+1\n",
    "                # frequency across this file\n",
    "                count = word_prob_this_file.get(word,0)\n",
    "                word_prob_this_file[word] = count+1\n",
    "                file_len += 1\n",
    "        fp.close()\n",
    "        # write the word distribution of this file [0-1] values\n",
    "        filename = FREQUENCY_PATH + pdf_file + \".txt\"\n",
    "        for word,count in word_prob_this_file.items():\n",
    "            prob = 1.0 * count / (1.0 * file_len)\n",
    "            word_prob_this_file[word] = prob\n",
    "        write_freqs(word_prob_this_file,filename)\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def write_freqs(word_freq,filename):\n",
    "    with open(filename, 'w+') as f:\n",
    "        for word, count in word_freq.items():\n",
    "            f.write('{} {}\\n'.format(word,count))\n",
    "\n",
    "def get_top_ten(word_freq):\n",
    "    heap = [(-value, key) for key, value in word_freq.items()]\n",
    "    largest = heapq.nsmallest(10, heap)\n",
    "    largest = [(key, -value) for value, key in largest]\n",
    "    return largest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_all_links()\n",
    "pdf_links = get_pdf_links(links)\n",
    "download_pdfs(pdf_links)\n",
    "word_freq = read_pdfs()\n",
    "write_freqs(word_freq,'word_freqs.txt')\n",
    "largest = get_top_ten(word_freq)\n",
    "print(largest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
