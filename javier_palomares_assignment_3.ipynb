{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Javier Palomares\n",
    "### Problem 1\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’. Focus on pages 1-19 (up\n",
    "to Part II), the remaining part is more relevant for communication.\n",
    "http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "Summarize what you learned briefly (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, Shannon introduces entropy within the context of communication systems. The paper first introduces the channel used for transmitting information in the system. The channel is a system where a sequence of choices from a finite set $ \\{s_1,s_2,...,s_n\\}$ are transmitted from one point to another.  \n",
    "The system has a capacity $ C=\\lim_{T\\to \\inf} \\frac{N(T)}{T}$ where $N(T)$ is the number of allowed signals of length $T$. If all sequences of $ \\{s_1,s_2,...,s_n\\}$ are allowed and the symbols have duration $t_1,t_2,...,t_n$ then it's shown that $C = logX_0$ where $X_0$ is the largest solution of $X^{-t_1} + X ^{-t_2} + ... + X ^{-t_n} =1$.  \n",
    "This result is then taken to mean that the log of the number of possible signals increases linearly with time, and the capacity of a channel can be determined by the rate of increase. More importantly, this also means that statistical knowledge of the prodution of the sequence has the effect of reducing the required capacity of the channel.  \n",
    "So we want to define a quantity which will measure how much inofrmation is produced by the source of the sequence, and at what rate this information is produced. This quantity, $H$, should have the following properties for a set of possible events each with probability $ p_1, p_2,...,p_n$:\n",
    "\n",
    "1. $H$ should be continuous in the $p_i$\n",
    "2. If all the $p_i$ are equal, then $H$ should be a monotonic function of n.\n",
    "3. If a choice can be broken down into 2 successive choices, then the original $H$ should be the weighted sum of the individual H. $H(\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}) = H(\\frac{1}{2},\\frac{1}{2}) + \\frac{1}{2} H (\\frac{2}{3},\\frac{1}{3})$ \n",
    "If turns out that the only $H$ satisfying the 3 properites is of the form  \n",
    "$H = - K \\sum_{i=1}^{n} p_i * log p_i, K>0$\n",
    "\n",
    "This is the mathematical definition of what we call entropy for a discrete system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Scraping, Entropy and ICML papers.\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2018 papers\n",
    "from http://proceedings.mlr.press/v80/.\n",
    "1. What are the top 10 common words in the ICML papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "URL = 'http://proceedings.mlr.press/v80/'\n",
    "DOWNLOAD_PATH = \"./pdfs/\"\n",
    "FREQUENCY_PATH = \"./freqs/\"\n",
    "os.mkdir(DOWNLOAD_PATH)\n",
    "os.mkdir(FREQUENCY_PATH)\n",
    "\n",
    "def get_all_links():\n",
    "    # connect to the url\n",
    "    website = urlopen(URL)\n",
    "    # get the html code\n",
    "    html = website.read().decode('utf8')\n",
    "\n",
    "    # use re.findall to get all the links\n",
    "    links = re.findall('\"((http|ftp)s?://.*?)\"', html)\n",
    "    return links\n",
    "\n",
    "def get_pdf_links(links):\n",
    "    pdfs = []\n",
    "    pattern = re.compile(\".*\\.pdf\")\n",
    "    for tuple in links:\n",
    "        link = tuple[0]\n",
    "        matches = re.match(pattern,link)\n",
    "        if matches:\n",
    "            pdfs.append(link)\n",
    "    return pdfs\n",
    "\n",
    "def download_pdfs(pdfs):\n",
    "    for pdf_link in pdfs:\n",
    "        pdf = urlopen(pdf_link)\n",
    "        o = urlparse(pdf_link)\n",
    "        print(\"Downloading {}\".format(pdf_link))\n",
    "        file_name = os.path.basename(o[2])\n",
    "        path = DOWNLOAD_PATH + file_name\n",
    "        f = open(path,'wb+')\n",
    "        f.write(pdf.read())\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def read_pdfs():\n",
    "    pdf_files = [f for f in listdir(DOWNLOAD_PATH) if isfile(join(DOWNLOAD_PATH, f))]\n",
    "    word_freq = {}\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopWords.add('et')\n",
    "    stopWords.add('al')\n",
    "    for pdf_file in pdf_files:\n",
    "        # word frequency in this one file. we'll write the\n",
    "        # word frequency of per every single file in addition to\n",
    "        # the word frequency across all files\n",
    "        word_prob_this_file= {}\n",
    "        file_len = 0\n",
    "        print(\"reading {}\".format(pdf_file))\n",
    "\n",
    "        path = DOWNLOAD_PATH + pdf_file\n",
    "        fp = open(path,'rb')\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "        # set parameters for analysis\n",
    "        laparams = LAParams()\n",
    "\n",
    "        # Create a PDFDevice object which translates interpreted information into desired format\n",
    "        # Device needs to be connected to resource manager to store shared resources\n",
    "        # device = PDFDevice(rsrcmgr)\n",
    "        # Extract the decive to page aggregator to get LT object elements\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "        # Create interpreter object to process page content from PDFDocument\n",
    "        # Interpreter needs to be connected to resource manager for shared resources and device\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            # As the interpreter processes the page stored in PDFDocument object\n",
    "            interpreter.process_page(page)\n",
    "            # The device renders the layout from interpreter\n",
    "            layout = device.get_result()\n",
    "            # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "            for lt_obj in layout:\n",
    "                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    text = lt_obj.get_text()\n",
    "                    # remove any characters that pdfminer was not able to\n",
    "                    # convert to unicode and converts to (cid:%%)\n",
    "                    text = re.sub('(cid:[0-9]+)','',text)\n",
    "                    # make all text lowercase\n",
    "                    text = text.lower()\n",
    "                    # remove line breaks\n",
    "                    text = text.replace('-\\n','')\n",
    "                    # remove all punctuation from the text\n",
    "                    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "                    # remove new line\n",
    "                    text = text.replace('\\n',' ')\n",
    "                    text = text.replace('−',' ')\n",
    "\n",
    "                    extracted_text += text\n",
    "        words = word_tokenize(extracted_text)\n",
    "\n",
    "        for word in words:\n",
    "            # don't include stop words or numeric words or single chars representing mathematical variables\n",
    "            if (word not in stopWords) and (word.isnumeric()==False) and (len(word) > 1):\n",
    "                # take all word to lowercase\n",
    "                word = word.lower()\n",
    "                # frequency across all files\n",
    "                count = word_freq.get(word,0)\n",
    "                word_freq[word] = count+1\n",
    "                # frequency across this file\n",
    "                count = word_prob_this_file.get(word,0)\n",
    "                word_prob_this_file[word] = count+1\n",
    "                file_len += 1\n",
    "        fp.close()\n",
    "        # write the word distribution of this file [0-1] values\n",
    "        filename = FREQUENCY_PATH + pdf_file + \".txt\"\n",
    "        for word,count in word_prob_this_file.items():\n",
    "            prob = 1.0 * count / (1.0 * file_len)\n",
    "            word_prob_this_file[word] = prob\n",
    "        write_freqs(word_prob_this_file,filename)\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def write_freqs(word_freq,filename):\n",
    "    with open(filename, 'w+') as f:\n",
    "        for word, count in word_freq.items():\n",
    "            f.write('{} {}\\n'.format(word,count))\n",
    "\n",
    "def get_top_ten(word_freq):\n",
    "    heap = [(-value, key) for key, value in word_freq.items()]\n",
    "    largest = heapq.nsmallest(10, heap)\n",
    "    largest = [(key, -value) for value, key in largest]\n",
    "    return largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learning', 0.006927446954772004), ('algorithm', 0.004573534712676739), ('model', 0.004241958823744521), ('data', 0.003597552300027319), ('function', 0.003541009238337302), ('using', 0.0034807785856675014), ('set', 0.0034061048683268814), ('training', 0.002723900537066896), ('neural', 0.0026630552859004653), ('gradient', 0.0026572166001824745)]\n"
     ]
    }
   ],
   "source": [
    "links = get_all_links()\n",
    "pdf_links = get_pdf_links(links)\n",
    "download_pdfs(pdf_links)\n",
    "word_freq = read_pdfs()\n",
    "write_freqs(word_freq,'word_freqs.txt')\n",
    "largest = get_top_ten(word_freq)\n",
    "print(largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top ten words are, in order:\n",
    "1. learning\n",
    "2. algorithm\n",
    "3. model\n",
    "4. data\n",
    "5. function\n",
    "6. using\n",
    "7. set\n",
    "8. training\n",
    "9. neural\n",
    "10. gradient\n",
    "\n",
    "Note: I threw out english stopwords as well as single character words I assumed represented mathematical variables, and any symbols that pdfminer was unable to convert to unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let Z be a randomly selected word in a randomly selected ICML paper. Estimate the entropy\n",
    "of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from alphabet_detector import AlphabetDetector\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "WORD_FREQ_FILE= 'word_freqs.txt'\n",
    "PARAGRAPH_NUM_WORDS = 30\n",
    "FREQUENCY_PATH = \"./freqs/\"\n",
    "\n",
    "K = 1.0\n",
    "def get_word_prob(all_words=False):\n",
    "    ad = AlphabetDetector()\n",
    "    word_probs = {}\n",
    "    total_word_count = 0\n",
    "    with open(WORD_FREQ_FILE,'r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            tokens = line.split()\n",
    "            word = tokens[0]\n",
    "            # Only consider latin words by default\n",
    "            if all_words or ad.only_alphabet_chars(word,\"LATIN\"):\n",
    "                count = int(tokens[1])\n",
    "                word_probs[word] = count\n",
    "                total_word_count += count\n",
    "            line = f.readline()\n",
    "    # convert the count to a prob by dividing by the total count\n",
    "    for word,count in word_probs.items():\n",
    "        word_probs[word] = count / total_word_count\n",
    "    return word_probs\n",
    "\n",
    "# return a list of the frequency a word per file\n",
    "def get_all_freq_files():\n",
    "    files = [f for f in listdir(FREQUENCY_PATH) if isfile(join(FREQUENCY_PATH, f))]\n",
    "\n",
    "    word_prob_all_files = []\n",
    "    for file in files:\n",
    "        file_word_prob = {}\n",
    "        path = FREQUENCY_PATH + file\n",
    "        with open(path,'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                tokens = line.split()\n",
    "                word = tokens[0]\n",
    "                prob = float(tokens[1])\n",
    "                file_word_prob[word] = prob\n",
    "                line = f.readline()\n",
    "        word_prob_all_files.append(file_word_prob)\n",
    "    return word_prob_all_files\n",
    "\n",
    "\n",
    "\n",
    "def compute_entropy_random_word_random_paper(words,word_prob_all_files):\n",
    "    H = 0\n",
    "    # iterate over all the words\n",
    "    num_words = len(words)\n",
    "    num_files = len(word_prob_all_files)\n",
    "    for word in words:\n",
    "        p = 0\n",
    "        # iterate over the word distribution per file\n",
    "        for word_prob in word_prob_all_files:\n",
    "            # all files are equally likely, so divide by the number of files\n",
    "            prob = word_prob.get(word,0) * 1.0 / num_files\n",
    "            p += prob\n",
    "        h = -1.0 * K * p * math.log(p,2)\n",
    "        H += h\n",
    "    return H\n",
    "\n",
    "def verify_word_probs(word_probs):\n",
    "    # verify the word probs sum up to 1.0\n",
    "    s = 0.0\n",
    "    for word,p in word_probs.items():\n",
    "        s += p\n",
    "    print(\"total prob: {}\".format(s))\n",
    "\n",
    "def first_order_synthesized_paragraph(word_probs,par_len):\n",
    "    # synthesize a paragraph of the given length with the\n",
    "    # given word probabilities\n",
    "    words = list(word_probs.keys())\n",
    "    weights = list(word_probs.values())\n",
    "    paragraph = np.random.choice(words,par_len,p=weights)\n",
    "    return ' '.join(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_probs = get_word_prob(True)\n",
    "words = all_word_probs.keys()\n",
    "word_prob_all_files = get_all_freq_files()\n",
    "H = compute_entropy_random_word_random_paper(words,word_prob_all_files)\n",
    "print(\"The computed entropy for the pdf text is {}\".format(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Synthesize a random paragraph using the marginal distribution over words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
